{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811edaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the GC-MS data from .MAT file\n",
    "def gcparser(mat):\n",
    "    \"\"\"\n",
    "    Extracts essential data from a Matlab formatted GCMS object loaded by sio.loadmat and wrangles this into a pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    mad (dict): Dictionary produced by loading a file using sio.loadmat\n",
    "\n",
    "    Returns: DataFrame: Total ion counts (TIC) arranged by samples (columns) and retention time (rows), with an additional 'Label' column indicating the class of each sample.\n",
    "\n",
    "    \"\"\"\n",
    "    data = np.transpose(mat['XTIC']) # Extract and transpose XTIC matrix\n",
    "    sample_names = mat['SAM'] # Extract sample names\n",
    "    sample_names = np.hstack(np.hstack(sample_names)).tolist() # Convert nested numpy arrays to list\n",
    "    RT = mat['RT'] # Extract retention time\n",
    "    RT = np.hstack(np.hstack(RT)).tolist() # Convert nested numpy arrays to list\n",
    "    y = mat['CLASS'] # Extract class labels\n",
    "    y = np.hstack(y).tolist() # Convert nested numpy arrays to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af35ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate SVM model\n",
    "def train_evaluate_svm(X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an SVM model on the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): Features (ion counts) for each sample.\n",
    "    y (Series): Labels indicating the class of each sample.\n",
    "\n",
    "    Returns:\n",
    "    svm_model (SVC): Trained SVM model.\n",
    "    \"\"\"\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X, y)\n",
    "    y_pred = svm_model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "    print(f'SVM Accuract: {accuracy}')\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.title('Confusion Matrix (SVM)')\n",
    "    plt.show()\n",
    "    return svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266607ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate Random Forest Model\n",
    "def train_evaluate_rf(X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a Random Forest model on the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): Features (ion counts) for each sample.\n",
    "    y (Series): Labels indicating the class of each sample.\n",
    "\n",
    "    Returns:\n",
    "    rf_model (RandomForestClassifier): Trained Random Forest model.\n",
    "    \"\"\"\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X, y)\n",
    "    y_pred = rf_model.predict(X)\n",
    "    accuracy = accuracy.score(y, y_pred)\n",
    "\n",
    "    print(f'Random Forest Accuracy: {accuracy}')\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.title('Confusion Matrix (Random Forest)')\n",
    "    plt.show()\n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f20d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap validation\n",
    "def bootstrap_validation(model, X, y, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Performs bootstrap validation on the provided model and data.\n",
    "\n",
    "    Parameters:\n",
    "    model: The machine learning model to validate.\n",
    "    X (DataFrame): Features (ion counts) for each sample.\n",
    "    y (Series): Labels indicating the class of each sample.\n",
    "    n_iterations (int): Number of bootstrap iterations.\n",
    "\n",
    "    Returns:\n",
    "    float: Average accuracy across all bootstrap iterations\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for _ in range(n_iterations):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    average_accuracy  np.mean(accuracies)\n",
    "    plt.hist(accuracies, bins=10)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Bootstrap Validation Accuracy Distribution')\n",
    "    plt.show()\n",
    "    return average_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform permutation testing\n",
    "def permutation_testing(model, X, y, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Performs permutation testing on the provided model and data.\n",
    "\n",
    "    Parameters:\n",
    "    model: The machine learning model to test.\n",
    "    X (DataFrame): Features (ion  counts) for each sample.\n",
    "    y (Series): Labels indicating the class of each sample.\n",
    "    n_iterations (int): Number of permutation iterations.\n",
    "\n",
    "    Returns:\n",
    "    float: Average accuracy across all permutation iterations.\n",
    "    \"\"\"\n",
    "    original_accuracies = []\n",
    "    perm_accuracies = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Original labels\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        original_accuracies.append(accuracy_score(y_test, y_pred=))\n",
    "\n",
    "        # Permuted labels\n",
    "        y_permuted = np.random.permutation(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_permuted, test_size=0.3, random_state=None)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        perm_accuracies.append(accuracy_score(y_test, y_pred=))\n",
    "    \n",
    "    average_perm_accuracy = np.mean(perm_accuracies)\n",
    "\n",
    "    plt.hist(original_accuracies, bins=10, alpha=.05, label='Original', color='blue')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Model Accuracy: Original vs Permuted')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    return average_perm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualise chromatograms\n",
    "def plot_chromatograms(df, title):\n",
    "    \"\"\"\n",
    "    Plots chromatograms for a subject of samples in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing ion counts and labels.\n",
    "    title (str): Title for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None.\n",
    "    \"\"\"\n",
    "    fix, ax = plt.subplots(figsize=(12, 8))\n",
    "    sample_indices = list(df[df['Label'] == 1].index[:5]) + list(df[df['Label'] == 2].index[:5])\n",
    "    subset_df = df.loc[sample_indices].drop('Label', axis=1)\n",
    "    subset_df.T.plot(ax=ax, legend=False)\n",
    "    ax.set_xlabel('Retention Time')\n",
    "    ax.set.ylabel('Ion Count')\n",
    "    ax.set_title(title)\n",
    "    plt.legend(title='Samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main workflow\n",
    "folders = ['blood', 'breath', 'faecal', 'urine']\n",
    "file_prefixes = {\n",
    "    'blood': 'BWG_BL'\n",
    "    'breath': 'BWG_BR'\n",
    "    'faecal': 'BWG_FA'\n",
    "    'urine': 'BWG_UR'\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for folder in folders:\n",
    "    prefix = file_prefixes[folder]\n",
    "    datasets = {\n",
    "        \"cd_all\": sio.loadmat(f'{folder}/{prefix}_CDvALL.mat'),\n",
    "        \"cd_ctrl\": sio.loadmat(f'{folder}/{prefix}_CDvCTRL.mat')\n",
    "    }\n",
    "\n",
    "    dataframes = {name: gcparser(mat) for name, mat in datasets.items()}\n",
    "\n",
    "    for name, df in dataframes.items():\n",
    "        print(f\"Processing {folder} dataset: {name}...\")\n",
    "        X = df.drop('Label', axis=1)\n",
    "        y = df['Label']\n",
    "\n",
    "        plot_chromatograms(df, f'Sample Chromatogram ({folder} - {name})')\n",
    "        print(f'Building and testing SVM classifier for {folder} - {name}...')\n",
    "        svm_model = train_evaluate_svm(X, y)\n",
    "\n",
    "        print(f\"Bootstrap validation for {folder} - {name}...\")\n",
    "        bootstrap_acc = bootstrap_validation(svm_model, X, y)\n",
    "        print(f'Bootstrap SVM Average Accuracy for {folder} - {name}: {bootstrap_acc}')\n",
    "\n",
    "        print(f\"Permutation testing for {folder} - {name}...\")\n",
    "        perm_acc = permutation_testing(svm_model, X, y)\n",
    "        print(f'Permutation Test SVM Average Accuracy for {folder} - {name}: {perm_acc}')\n",
    "\n",
    "        results.append((f\"{folder} - {name}\", 'SVM', 'Bootstrap', bootstrap_acc))\n",
    "        results.append((f\"{folder} - {name}\", 'SVM', 'Permutation', perm_acc))\n",
    "\n",
    "        print(f\"Building and testing Random Forest classifier for {folder} - {name}...\")\n",
    "        rf_model = train_evaluate_rf(X, y)\n",
    "\n",
    "        print(f\"Bootstrap validation for {folder} - {name}...\")\n",
    "        bootstrap_acc_rf = bootstrap_validation(rf_model, X, y)\n",
    "        print(f'Bootstrap RF Average Accuracy for {folder} - {name}: {bootstrap_acc_rf}')\n",
    "\n",
    "        print(f\"Permutation testing for {folder} - {name}...\")\n",
    "        perm_acc_rf = permutation_testing(rf_model, X, y)\n",
    "        print(f'Permutation Test RF Average Accuracy for {folder} - {name}: {perm_acc_rf}')\n",
    "\n",
    "        results.append((f\"{folder} - {name}\", 'Random Forest', 'Bootstrap', bootstrap_acc_rf))\n",
    "        results.append((f\"{folder} - {name}\", 'Random Forest', 'Permutation', perm_acc_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results in a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Dataset', 'Model', 'Validation Method', 'Accuracy'])\n",
    "\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
